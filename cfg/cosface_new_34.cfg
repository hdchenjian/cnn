[network]
batch=5
height=112
width=112
#width=96
channels=3
max_batches=30000

learning_rate=0.1
#policy=poly
#learning_rate_poly_power=4
policy=steps
steps=15000,21000,24000
scales=.1,.1,.1

momentum=0.9
decay=0.0005

#saturation = 1.02
#exposure = 1.05
#hue=.02

flip=1
mean_value=127.5
scale=0.0078125

# get 512 dimension feature
output_layer = 76
classes = 5000
#classes = 100
accuracy_count_max=8000

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

#stage 1 unit 1
[batchnorm]

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
weight_filler=xavier
activation=linear

[route]
layers=-4

[convolutional]
batch_normalize=1
filters=64
size=1
stride=2
pad=0
weight_filler=xavier
activation=linear

[shortcut]
from=-3

#stage 1 unit 2
[batchnorm]

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

[shortcut]
from=-4

#stage 1 unit 3
[batchnorm]

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

[shortcut]
from=-4

#stage 2 unit 1
[batchnorm]

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=128
size=3
stride=2
pad=1
weight_filler=xavier
activation=linear

[route]
layers=-4

[convolutional]
batch_normalize=1
filters=128
size=1
stride=2
pad=0
weight_filler=xavier
activation=linear

# _plus 3
[shortcut]
from=-3

#stage 2 unit 2
[batchnorm]

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

# _plus 4
[shortcut]
from=-4

#stage 2 unit 3
[batchnorm]

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

# _plus 5
[shortcut]
from=-4

#stage 2 unit 4
[batchnorm]

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

# _plus 6
[shortcut]
from=-4

#stage 3 unit 1
[batchnorm]

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=256
size=3
stride=2
pad=1
weight_filler=xavier
activation=linear

[route]
layers=-4

[convolutional]
batch_normalize=1
filters=256
size=1
stride=2
pad=0
weight_filler=xavier
activation=linear

# _plus 7
[shortcut]
from=-3

#stage 3 unit 2
[batchnorm]

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

# _plus 8
[shortcut]
from=-4

#stage 3 unit 3
[batchnorm]

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

# _plus 9
[shortcut]
from=-4

#stage 3 unit 4
[batchnorm]

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

# _plus 10
[shortcut]
from=-4

#stage 3 unit 5
[batchnorm]

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

# _plus11
[shortcut]
from=-4

#stage 3 unit 6
[batchnorm]

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

# _plus12
[shortcut]
from=-4

#stage4 unit 1
[batchnorm]

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
weight_filler=xavier
activation=linear

[route]
layers=-4

[convolutional]
batch_normalize=1
filters=512
size=1
stride=2
pad=0
weight_filler=xavier
activation=linear

#_plus21
[shortcut]
from=-3

#stage 4 unit 2
[batchnorm]

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

#_plus22
[shortcut]
from=-4

#stage 4 unit 3
[batchnorm]

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
weight_filler=xavier
activation=prelu

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
weight_filler=xavier
activation=linear

#_plus23
[shortcut]
from=-4

[batchnorm]

[dropout]
probability=.4

[connected]
batch_normalize=0
output = 512
weight_filler=xavier
activation=linear

[batchnorm]

[normalize]

##########################

[connected]
#output = 100
output = 5000
lr_mult=1
lr_decay_mult=0
bias_mult=0
bias_decay_mult=0

weight_filler=xavier
weight_normalize=1
bias_term=0
activation=linear

[softmax]
label_specific_margin_bias=-0.35
margin_scale=64
