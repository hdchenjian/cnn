[network]
batch=200
height=112
width=96
channels=3
max_batches=800

learning_rate=0.05
policy=poly
learning_rate_poly_power=3
#policy=steps
#steps=30,100,200
#scales=.1,.1,.1

momentum=0.9
decay=0.0005

#hue=0.01
#saturation=0.99
#exposure=0.95

flip=1
mean_value=127.5
scale=0.0078125

# get 512 dimension feature
output_layer = 28
classes = 4038

[convolutional]
filters=64
size=3
stride=2
pad=0
weight_filler=xavier
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=2
bias_decay_mult=0

[convolutional]
filters=64
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[convolutional]
filters=64
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[shortcut]
from=-3

# conv 2
[convolutional]
filters=128
size=3
stride=2
pad=0
weight_filler=xavier
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=2
bias_decay_mult=0

[convolutional]
filters=128
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[convolutional]
filters=128
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[shortcut]
from=-3

[convolutional]
filters=128
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[convolutional]
filters=128
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[shortcut]
from=-3

# conv 3
[convolutional]
filters=256
size=3
stride=2
pad=0
weight_filler=xavier
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=2
bias_decay_mult=0

[convolutional]
filters=256
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[convolutional]
filters=256
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[shortcut]
from=-3

[convolutional]
filters=256
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[convolutional]
filters=256
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[shortcut]
from=-3

[convolutional]
filters=256
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[convolutional]
filters=256
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[shortcut]
from=-3

[convolutional]
filters=256
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[convolutional]
filters=256
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[shortcut]
from=-3


# conv 4
[convolutional]
filters=512
size=3
stride=2
pad=0
weight_filler=xavier
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=2
bias_decay_mult=0

[convolutional]
filters=512
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[convolutional]
filters=512
size=3
stride=1
pad=0
weight_filler=gaussian
weight_filler_std=0.01
activation=relu
lr_mult=1
lr_decay_mult=1
bias_mult=0
bias_decay_mult=0

[shortcut]
from=-3

[connected]
lr_mult=1
lr_decay_mult=1
bias_mult=2
bias_decay_mult=0

output = 512
weight_filler=xavier
activation=linear

#[normalize]
######################################################

[connected]
output = 4038
lr_mult=1
lr_decay_mult=0
bias_mult=2
bias_decay_mult=0
weight_filler=xavier
#weight_normalize=1
#bias_term=0
activation=linear

[softmax]
#label_specific_margin_bias=-0.39
#margin_scale=30
